//------------------------------------------------------------------------------
//
// Copyright 2025 Jiri Bobek. All rights reserved.
// License: GPL 3.0 or later. See LICENSE.txt for details.
//
//------------------------------------------------------------------------------

use super::super::linear::*;

use crate::ErrPack;
use crate::autograd::{self, AutogradNode, GradientCapture};
use crate::nn::ModelContext;
use crate::nn::layers::Layer;
use crate::tensor::device::cpu::CPUDevice;
use crate::tensor::math::approx_eq;
use crate::tensor::{self, HasDType, Tensor, TensorOpError};

// Note: The expected tensors were generated by gen_test_data.py

#[allow(clippy::panic_in_result_fn)]
#[allow(clippy::unwrap_used)]
#[test]
fn test_linear() -> Result<(), ErrPack<TensorOpError>> {
	let dev = CPUDevice::new();
	let mut model_ctx = ModelContext::new(dev.clone());
	let linear = Linear::new(5, 6, f32::dtype, &mut model_ctx)?;
	model_ctx.init_optimizer()?;

	let lit = Tensor::literal_factory::<f32>(dev);

	#[rustfmt::skip] let weights = lit.new_2d(&[
		[-0.7392, -0.4243, -2.2199, -0.7662, -0.4344],
		[-1.1176, -0.5131,  0.5884,  1.6860, -0.5456],
		[-0.6692, -0.7482, -0.5937, -0.4305, -1.6972],
		[ 1.0881, -0.7972, -1.2000, -0.6788, -0.9008],
		[ 1.8882, -1.1999,  0.3821, -0.2152,  0.2094],
		[-1.1796, -1.8167,  1.2314, -0.6760,  0.0761],
	])?;

	#[rustfmt::skip] linear.weights().borrow().value().assign(&weights)?;

	#[rustfmt::skip] let inp = lit.new_2d(&[
		[-1.2794, -0.1038,  0.3636, -0.0918, -0.6903],
		[-0.2495, -1.7407, -0.4136,  1.2375,  0.0408],
		[ 0.2060, -1.0269,  0.2663,  1.8425,  1.4105],
		[-1.8738,  1.0913,  0.5786, -0.8210,  0.0362],
	])?;

	#[rustfmt::skip] let expected_out = lit.new_2d(&[
		[ 0.2472,  0.8582,  0.8627, -0.4747, -1.0183,  0.9638],
		[ 0.3914,  1.3384,  0.4977,  0.3290,  0.5374,  0.9454],
		[-1.0430,  1.2478, -1.2141, -0.8041,  0.7253,  0.3633],
		[ 0.1122,  0.2105,  0.1726, -1.3767, -1.9866,  0.6699],
	])?;

	let d_inp_capture = GradientCapture::new();
	let d_inp = d_inp_capture.storage();
	let out = linear.forward(AutogradNode::new(inp, Some(d_inp_capture)))?;
	let (out, backward_fn) = out.take();

	println!("out = {}", out.borrow()?.view::<f32>()?);
	println!("expected_out = {}", expected_out.borrow()?.view::<f32>()?);

	assert!(approx_eq(&out, &expected_out, 1e-4)?);

	#[rustfmt::skip] let d_out = lit.new_2d(&[
		[-1.2192,  0.9470, -1.0698,  1.0365,  0.1644, -0.1481],
		[-1.0424, -0.4814, -1.5834,  0.4658,  1.0362, -0.2995],
		[-0.5644,  1.4450, -1.0186, -0.5245,  2.2684, -0.5567],
		[-0.9963, -1.7835,  0.6185,  2.0077, -0.5136, -0.9927],
	])?;

	#[rustfmt::skip] let expected_d_inp = lit.new_2d(&[
		[ 2.1717,  0.0773,  2.5355,  2.3525,  0.9181],
		[ 5.1848,  0.8034,  2.4390,  0.3320,  3.1774],
		[ 3.8531, -1.0322,  3.5185,  3.5514,  2.0907],
		[ 4.7016,  1.6943, -3.0328, -3.0911, -1.6355],
	])?;

	#[rustfmt::skip] let expected_d_weights = lit.new_2d(&[
		[ 3.5705,  1.4334, -0.7389, -1.4000, -0.0331],
		[ 2.5481, -2.6905, -0.1037,  3.4440,  1.3003],
		[ 0.3950,  4.5882,  0.3525, -4.2458, -0.7405],
		[-5.3124,  1.8112,  1.2062, -2.1334, -1.3636],
		[ 0.9608, -4.7107, -0.0619,  5.8684,  3.1098],
		[ 2.0096,  0.0251, -0.6526, -0.5677, -0.7311],
	])?;

	autograd::run(backward_fn, d_out)?;
	let d_inp = d_inp.borrow_mut().take().unwrap();

	println!("d_inp = {}", d_inp.borrow()?.view::<f32>()?);
	println!("expected_d_inp = {}", expected_d_inp.borrow()?.view::<f32>()?);

	let weights = linear.weights();
	let weights = weights.borrow();
	let grad = weights.grad().unwrap();
	println!("d_weights = {}", grad.borrow()?.view::<f32>()?);

	assert!(approx_eq(&d_inp, &expected_d_inp, 1e-4)?);
	assert!(approx_eq(grad, &expected_d_weights, 1e-4)?);

	Ok(())
}

#[allow(clippy::panic_in_result_fn)]
#[allow(clippy::unwrap_used)]
#[test]
fn test_multihead_linear() -> Result<(), ErrPack<tensor::TensorOpError>> {
	let dev = CPUDevice::new();
	let mut model_ctx = ModelContext::new(dev.clone());
	let linear = MultiheadLinear::new(5, 3, 2, f32::dtype, &mut model_ctx)?;
	model_ctx.init_optimizer()?;

	let lit = Tensor::literal_factory::<f32>(dev);

	#[rustfmt::skip] let weights = lit.new_2d(&[
		[-0.7392, -0.4243, -2.2199, -0.7662, -0.4344],
		[-1.1176, -0.5131,  0.5884,  1.6860, -0.5456],
		[-0.6692, -0.7482, -0.5937, -0.4305, -1.6972],
		[ 1.0881, -0.7972, -1.2000, -0.6788, -0.9008],
		[ 1.8882, -1.1999,  0.3821, -0.2152,  0.2094],
		[-1.1796, -1.8167,  1.2314, -0.6760,  0.0761],
	])?;

	#[rustfmt::skip] linear.linear.weights().borrow().value().assign(&weights)?;

	#[rustfmt::skip] let inp = lit.new_2d(&[
		[-1.2794, -0.1038,  0.3636, -0.0918, -0.6903],
		[-0.2495, -1.7407, -0.4136,  1.2375,  0.0408],
		[ 0.2060, -1.0269,  0.2663,  1.8425,  1.4105],
		[-1.8738,  1.0913,  0.5786, -0.8210,  0.0362],
	])?;

	#[rustfmt::skip] let expected_out = lit.new_3d(&[
		[ [ 0.2472,  0.8582,  0.8627], [-0.4747, -1.0183,  0.9638] ],
		[ [ 0.3914,  1.3384,  0.4977], [ 0.3290,  0.5374,  0.9454] ],
		[ [-1.0430,  1.2478, -1.2141], [-0.8041,  0.7253,  0.3633] ],
		[ [ 0.1122,  0.2105,  0.1726], [-1.3767, -1.9866,  0.6699] ],
	])?;

	let d_inp_capture = GradientCapture::new();
	let d_inp = d_inp_capture.storage();
	let out = linear.forward(AutogradNode::new(inp, Some(d_inp_capture)))?;
	let (out, backward_fn) = out.take();

	println!("out = {}", out.borrow()?.view::<f32>()?);
	println!("expected_out = {}", expected_out.borrow()?.view::<f32>()?);

	assert!(approx_eq(&out, &expected_out, 1e-4)?);

	#[rustfmt::skip] let d_out = lit.new_3d(&[
		[ [-1.2192,  0.9470, -1.0698], [ 1.0365,  0.1644, -0.1481] ],
		[ [-1.0424, -0.4814, -1.5834], [ 0.4658,  1.0362, -0.2995] ],
		[ [-0.5644,  1.4450, -1.0186], [-0.5245,  2.2684, -0.5567] ],
		[ [-0.9963, -1.7835,  0.6185], [ 2.0077, -0.5136, -0.9927] ],
	])?;

	#[rustfmt::skip] let expected_d_inp = lit.new_2d(&[
		[ 2.1717,  0.0773,  2.5355,  2.3525,  0.9181],
		[ 5.1848,  0.8034,  2.4390,  0.3320,  3.1774],
		[ 3.8531, -1.0322,  3.5185,  3.5514,  2.0907],
		[ 4.7016,  1.6943, -3.0328, -3.0911, -1.6355],
	])?;

	autograd::run(backward_fn, d_out)?;
	let d_inp = d_inp.borrow_mut().take().unwrap();

	println!("d_inp = {}", d_inp.borrow()?.view::<f32>()?);
	println!("expected_d_inp = {}", expected_d_inp.borrow()?.view::<f32>()?);

	assert!(approx_eq(&d_inp, &expected_d_inp, 1e-4)?);
	Ok(())
}
