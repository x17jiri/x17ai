//------------------------------------------------------------------------------
//
// Copyright 2025 Jiri Bobek. All rights reserved.
// License: GPL 3.0 or later. See LICENSE.txt for details.
//
//------------------------------------------------------------------------------

use super::super::linear::*;

use crate::ErrPack;
use crate::autograd::{self, AutogradNode, GradientCapture};
use crate::nn::ModelContext;
use crate::nn::layers::Layer;
use crate::nn::layers::wrapper::Wrapper;
use crate::tensor::device::cpu::CPUDevice;
use crate::tensor::math::approx_eq;
use crate::tensor::{HasDType, Tensor, TensorOpError};

//--------------------------------------------------------------------------------------------------

// Note: The expected tensors were generated by gen_test_data.py

#[allow(clippy::panic_in_result_fn)]
#[allow(clippy::unwrap_used)]
#[test]
fn test_wrapper() -> Result<(), ErrPack<TensorOpError>> {
	let dev = CPUDevice::new();
	let mut model_ctx = ModelContext::new(dev.clone());
	let linear = Linear::new(4, 4, f32::dtype, &mut model_ctx)?;
	let wrapper = Wrapper::new(linear, 1.0e-5).unwrap();
	model_ctx.init_optimizer()?;

	let lit = Tensor::literal_factory::<f32>(dev);

	#[rustfmt::skip] let weights = lit.new_2d(&[
		[ 1.7020,  6.0947,  3.4361, -0.6843],
		[-6.3394,  1.0607,  2.0116,  0.4654],
		[ 0.5924, -2.0077, -1.3039,  0.0213],
		[-2.1613,  1.1771, -5.4062, -0.2243],
	])?;

	#[rustfmt::skip] wrapper.nested.weights().borrow().value().assign(&weights)?;

	#[rustfmt::skip] let inp = lit.new_2d(&[
		[ 1.1364, -1.0560,  2.3672, -2.8698],
		[-0.4039,  0.3338,  0.2042,  0.1764],
		[ 3.5515,  0.7146, -0.4302, -0.3935],
		[ 1.0369,  0.2028, -0.1517,  0.3535],
		[-0.6558,  2.5798,  0.4381, -1.5726],
	])?;

	#[rustfmt::skip] let expected_out = lit.new_2d(&[
		[ 2.5247, -2.2712,  2.2793, -6.8029],
		[ 2.8671,  6.1149, -1.7842,  0.3840],
		[ 6.0563, -5.5004, -0.0972, -1.5983],
		[ 3.0270, -5.5770,  0.2151, -0.7684],
		[ 4.8487,  4.8354, -1.5387, -0.7917],
	])?;

	let d_inp_capture = GradientCapture::new();
	let d_inp = d_inp_capture.storage();
	let out = wrapper.forward(AutogradNode::new(inp, Some(d_inp_capture)))?;
	let (out, backward_fn) = out.take();

	println!("out = {}", out.borrow()?.view::<f32>()?);
	println!("expected_out = {}", expected_out.borrow()?.view::<f32>()?);

	assert!(approx_eq(&out, &expected_out, 1e-4)?);

	/*
	#[rustfmt::skip] let d_out = lit.new_2d(&[
		[-1.2192,  0.9470, -1.0698,  1.0365,  0.1644, -0.1481],
		[-1.0424, -0.4814, -1.5834,  0.4658,  1.0362, -0.2995],
		[-0.5644,  1.4450, -1.0186, -0.5245,  2.2684, -0.5567],
		[-0.9963, -1.7835,  0.6185,  2.0077, -0.5136, -0.9927],
	])?;

	#[rustfmt::skip] let expected_d_inp = lit.new_2d(&[
		[ 2.1717,  0.0773,  2.5355,  2.3525,  0.9181],
		[ 5.1848,  0.8034,  2.4390,  0.3320,  3.1774],
		[ 3.8531, -1.0322,  3.5185,  3.5514,  2.0907],
		[ 4.7016,  1.6943, -3.0328, -3.0911, -1.6355],
	])?;

	#[rustfmt::skip] let expected_d_weights = lit.new_2d(&[
		[ 3.5705,  1.4334, -0.7389, -1.4000, -0.0331],
		[ 2.5481, -2.6905, -0.1037,  3.4440,  1.3003],
		[ 0.3950,  4.5882,  0.3525, -4.2458, -0.7405],
		[-5.3124,  1.8112,  1.2062, -2.1334, -1.3636],
		[ 0.9608, -4.7107, -0.0619,  5.8684,  3.1098],
		[ 2.0096,  0.0251, -0.6526, -0.5677, -0.7311]
	])?;

	autograd::run(backward_fn, d_out)?;
	let d_inp = d_inp.borrow_mut().take().unwrap();

	println!("d_inp = {}", d_inp.borrow()?.view::<f32>()?);
	println!("expected_d_inp = {}", expected_d_inp.borrow()?.view::<f32>()?);

	let weights = wrapper.nested.weights();
	let weights = weights.borrow();
	let grad = weights.grad().unwrap();
	println!("d_weights = {}", grad.borrow()?.view::<f32>()?);

	assert!(approx_eq(&d_inp, &expected_d_inp, 1e-4)?);
	assert!(approx_eq(grad, &expected_d_weights, 1e-4)?);
	*/

	Ok(())
}

//--------------------------------------------------------------------------------------------------
