//------------------------------------------------------------------------------
//
// Copyright 2025 Jiri Bobek. All rights reserved.
// License: GPL 3.0 or later. See LICENSE.txt for details.
//
//------------------------------------------------------------------------------

use super::super::attention::*;

use crate::ErrPack;
use crate::autograd::AutogradTensor;
use crate::tensor::device::cpu::CPUDevice;
use crate::tensor::math::approx_eq;
use crate::tensor::{Tensor, TensorOpError};

// Note: The expected tensors were generated by gen_test_data.py

#[allow(clippy::panic_in_result_fn)]
#[allow(clippy::unwrap_used)]
#[test]
fn test_attention() -> Result<(), ErrPack<TensorOpError>> {
	let attention = Attention::new();
	let dev = CPUDevice::new();

	#[rustfmt::skip] let q = Tensor::literal_factory::<f32>(dev.clone()).new_3d(&[
		[[-0.0079,  0.2013,  1.2039,  0.2116,  0.0068, -2.5852, -2.0240,  0.7858]],
		[[-0.8002,  0.1333,  0.9970, -0.8524, -0.4262,  1.2937,  1.5938, -0.5669]],
		[[ 0.6690,  1.2581,  0.1335, -0.3066,  0.6481, -0.9801, -0.0451, -0.3624]],
		[[ 1.6529,  0.9804,  0.0835, -1.5914, -0.1760,  1.2267, -0.5947,  0.7530]],
	])?;
	#[rustfmt::skip] let k = Tensor::literal_factory::<f32>(dev.clone()).new_3d(&[
		[[ 0.4313,  0.1423, -0.5095, -0.3916,  0.4594, -0.0961, -1.5407, -0.8292]],
		[[-1.0260, -0.4181,  0.5024, -1.1891, -1.9262,  1.2001, -1.4354, -1.3418]],
		[[-1.0642, -0.4961,  0.5398, -0.0256,  0.3406,  1.1670, -1.6464,  1.7155]],
		[[ 0.5668,  0.0046, -0.2088,  0.2224, -0.6140,  0.2493,  0.7993, -0.3674]],
		[[-0.5676,  0.2787,  0.5112, -2.2847, -0.9451,  0.2795,  1.0961,  1.9307]],
		[[-0.6721,  1.8384, -0.9810,  0.8848,  0.3362, -0.0636, -0.5931, -1.8145]],
		[[-0.7325,  0.0744, -0.2374, -0.6517,  0.1745, -0.0515, -0.7095,  0.4936]],
	])?;
	#[rustfmt::skip] let v = Tensor::literal_factory::<f32>(dev.clone()).new_3d(&[
		[[ 0.2457, -0.4612,  1.6512,  1.2265, -0.9842, -0.5971, -0.0895, -0.9303]],
		[[-1.0694, -1.6812,  1.5465,  1.3195, -1.0441,  0.1058, -0.3940, -1.4757]],
		[[-1.0756, -0.3880,  0.6743, -1.2219, -0.2558, -0.0998,  0.3268,  0.2325]],
		[[-0.5794,  0.5666, -0.8820, -1.5885, -0.5548, -0.5757, -0.3794, -1.0277]],
		[[-1.8045,  1.7775,  0.5436, -2.3099,  1.2602, -0.4861, -0.7186, -0.9591]],
		[[-2.6585, -0.3530, -1.0972, -0.4793, -0.0963, -0.3660, -0.8102,  0.2133]],
		[[-0.4956,  0.7299,  0.4323, -0.7839, -0.3641, -0.1709,  0.2184,  0.4475]],
	])?;

	#[rustfmt::skip] let expected_out = Tensor::literal_factory::<f32>(dev.clone()).new_3d(&[
		[[-0.5518, -0.1719,  0.9235, -0.2605, -0.5154, -0.2914,  0.1115, -0.1630]],
		[[-1.6000,  0.9751,  0.7090, -1.4884,  0.6884, -0.3585, -0.6294, -1.0568]],
		[[-1.8142, -0.2372, -0.3767, -0.2228, -0.2896, -0.4150, -0.5811, -0.1087]],
		[[-1.5914,  1.3567,  0.6289, -1.8655,  0.9071, -0.4398, -0.5990, -0.8973]],
	])?;

	let out = attention.forward(
		AutogradTensor::new(q, None),
		AutogradTensor::new(k, None),
		AutogradTensor::new(v, None),
	)?;
	let (out, _backward_fn) = out.into_parts();

	println!("out = {}", out.borrow()?.view::<f32>()?);
	println!("expected_out = {}", expected_out.borrow()?.view::<f32>()?);

	assert!(approx_eq(&out, &expected_out, 1e-4)?);
	Ok(())
}
