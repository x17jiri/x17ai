//------------------------------------------------------------------------------
//
// Copyright 2025 Jiri Bobek. All rights reserved.
// License: GPL 3.0 or later. See LICENSE.txt for details.
//
//------------------------------------------------------------------------------

use super::super::linear::*;

use crate::ErrPack;
use crate::autograd::{self, AutogradTensor, GradientCapture};
use crate::nn::ModelContext;
use crate::nn::fragments::UnaryFragment;
use crate::nn::fragments::wrapper::Wrapper;
use crate::tensor::device::cpu::CPUDevice;
use crate::tensor::math::approx_eq;
use crate::tensor::{HasDType, Tensor, TensorOpError};

//--------------------------------------------------------------------------------------------------

// Note: The expected tensors were generated by gen_test_data.py

#[allow(clippy::panic_in_result_fn)]
#[allow(clippy::unwrap_used)]
#[allow(clippy::approx_constant)]
#[test]
fn test_wrapper() -> Result<(), ErrPack<TensorOpError>> {
	let dev = CPUDevice::new();
	let mut model_ctx = ModelContext::new(dev.clone());
	let linear = Linear::new(4, 4, f32::dtype, &mut model_ctx)?;
	let wrapper = Wrapper::new(linear, 1.0e-5);
	model_ctx.init_optimizer()?;

	let lit = Tensor::literal_factory::<f32>(dev);

	#[rustfmt::skip] let weights = lit.new_2d(&[
		[ 0.4410,  1.0766,  3.7616, -6.4873],
		[ 9.6167,  1.4782, -1.6646, -9.1857],
		[ 1.6518,  1.8148,  0.7772, -3.1704],
		[-3.5822,  5.1029,  0.5198, -2.1559],
	])?;

	#[rustfmt::skip] wrapper.nested.weights().borrow().value().assign(&weights)?;

	#[rustfmt::skip] let inp = lit.new_2d(&[
		[-2.7016,  2.2977, -1.3401,  0.8465],
		[ 0.1577,  1.2480,  2.9994, -1.7373],
		[-1.9980,  3.0446,  5.2757, -1.2850],
		[-2.1982, -0.8580, -0.6265, -1.6002],
		[-2.2695, -1.1619, -3.1626,  2.6974],
	])?;

	#[rustfmt::skip] let expected_out = lit.new_2d(&[
		[-5.0828, -4.9433, -2.3744,  5.7069],
		[ 6.6571,  5.1333,  5.8101,  1.2751],
		[ 2.6789,  1.2563,  6.8663,  3.0292],
		[-0.0979, -3.1415, -0.8326,  0.6677],
		[-8.7594, -9.9914, -6.6216,  1.6190]
	])?;

	let d_inp_capture = GradientCapture::new();
	let d_inp = d_inp_capture.storage();
	let out = wrapper.forward(AutogradTensor::new(inp, Some(d_inp_capture)))?;
	let (out, backward_fn) = out.into_parts();

	println!("out = {}", out.borrow()?.view::<f32>()?);
	println!("expected_out = {}", expected_out.borrow()?.view::<f32>()?);

	assert!(approx_eq(&out, &expected_out, 1e-4)?);

	#[rustfmt::skip] let d_out = lit.new_2d(&[
		[-0.3847,  0.0664, -0.6217,  0.5983],
		[ 0.2775,  0.2007, -0.7973,  0.0167],
		[ 0.5394, -0.0854,  0.5947, -0.4340],
		[ 0.0528, -0.1087,  0.2953,  0.3368],
		[ 0.1521, -0.4394, -0.6771, -0.0617],
	])?;

	#[rustfmt::skip] let expected_d_inp = lit.new_2d(&[
		[-0.5328,  0.2676, -0.4560,  0.5775],
		[ 0.4667, -0.3230, -0.2671, -0.5997],
		[ 0.4608, -0.1094,  0.5330, -0.5761],
		[-0.2135,  0.2325,  0.3258,  0.0970],
		[-0.4608, -0.3881, -0.1978,  0.5257],
	])?;

	#[rustfmt::skip] let expected_d_weights = lit.new_2d(&[
		[ 0.0080,  0.1316,  1.3674, -0.5308],
		[ 0.5497,  0.4082,  0.7595, -0.4936],
		[ 0.6186, -0.5722,  0.8431, -0.8263],
		[-1.0153,  0.1463, -1.1506, -0.0219]
	])?;

	autograd::run(backward_fn, d_out)?;
	let d_inp = d_inp.borrow_mut().take().unwrap();

	println!("d_inp = {}", d_inp.borrow()?.view::<f32>()?);
	println!("expected_d_inp = {}", expected_d_inp.borrow()?.view::<f32>()?);

	let weights = wrapper.nested.weights();
	let weights = weights.borrow();
	let grad = weights.grad().unwrap();
	println!("d_weights = {}", grad.borrow()?.view::<f32>()?);

	assert!(approx_eq(&d_inp, &expected_d_inp, 1e-4)?);
	assert!(approx_eq(grad, &expected_d_weights, 1e-4)?);

	Ok(())
}

//--------------------------------------------------------------------------------------------------
